{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "- Rasa core and Rasa NLU combined is called RASA STACK\n",
    "- We will build a weather reporting chatbot\n",
    "    - this will do Entity Extraction and Intent classification task\n",
    "    \n",
    "### Setup\n",
    "1. pip install -r requirements.txt\n",
    "2. language model ( en ) \n",
    "    - lanuage model is used to parse incoming text messages and extract necessary information\n",
    "3. Rasa NLU trainer\n",
    "    - this makes generating training data lot easier\n",
    "    - this is a UI, and js based application, so we will need npm and nodejs\n",
    "    - download node js with npm and add to path\n",
    "    - then run\n",
    "        - npm i -g rasa-nlu-trainer\n",
    "    - this will install rasa-nlu trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import logging, io, json, warnings\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def pprint(o):\n",
    "    # small helper to make dict dumps a bit prettier\n",
    "    print(json.dumps(o, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 92.1MB/s ta 0:00:01   2% |▊                               | 860kB 5.1MB/s eta 0:00:08    4% |█▎                              | 1.5MB 30.3MB/s eta 0:00:02    4% |█▌                              | 1.7MB 33.4MB/s eta 0:00:02    4% |█▋                              | 1.9MB 2.6MB/s eta 0:00:14/s eta 0:00:01    17% |█████▌                          | 6.4MB 6.9MB/s eta 0:00:05    18% |██████                          | 7.0MB 11.3MB/s eta 0:00:03    21% |██████▉                         | 8.0MB 1.4MB/s eta 0:00:21    23% |███████▌                        | 8.8MB 1.7MB/s eta 0:00:17    29% |█████████▌                      | 11.1MB 5.1MB/s eta 0:00:06    33% |██████████▊                     | 12.5MB 8.9MB/s eta 0:00:03    39% |████████████▌                   | 14.6MB 6.7MB/s eta 0:00:04    39% |████████████▋                   | 14.8MB 2.1MB/s eta 0:00:11    43% |██████████████                  | 16.4MB 3.5MB/s eta 0:00:06    44% |██████████████▏                 | 16.5MB 2.1MB/s eta 0:00:10    55% |█████████████████▊              | 20.7MB 8.0MB/s eta 0:00:03    57% |██████████████████▎             | 21.3MB 6.0MB/s eta 0:00:03    61% |███████████████████▌            | 22.8MB 4.4MB/s eta 0:00:04    62% |████████████████████            | 23.3MB 5.1MB/s eta 0:00:03    64% |████████████████████▋           | 24.1MB 3.0MB/s eta 0:00:05    71% |██████████████████████▉         | 26.7MB 3.1MB/s eta 0:00:04    74% |███████████████████████▊        | 27.8MB 7.7MB/s eta 0:00:02    77% |████████████████████████▋       | 28.8MB 1.8MB/s eta 0:00:05    77% |█████████████████████████       | 29.1MB 5.0MB/s eta 0:00:02    80% |█████████████████████████▋      | 29.9MB 7.3MB/s eta 0:00:02    89% |████████████████████████████▋   | 33.4MB 3.4MB/s eta 0:00:02    91% |█████████████████████████████▎  | 34.2MB 1.2MB/s eta 0:00:03    91% |█████████████████████████████▍  | 34.3MB 2.0MB/s eta 0:00:02\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/vivek/anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /home/vivek/anaconda3/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. install language model \n",
    "\n",
    "import sys\n",
    "python = sys.executable\n",
    "\n",
    "# this will download english spacy model\n",
    "    # it will install it and reference it to abbreviation en\n",
    "!{python} -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "- we have to teach a chatbot how to understand human unstructured language so that bot will understand what we are saying\n",
    "    - So we will train NLU model which will take unstructured text messages and will return structured data in the form of intents and entities which our bot will understand\n",
    "- Intent\n",
    "    - what the message is about\n",
    "- Entity\n",
    "    - informations like location names, dates etc\n",
    "    - this helps chatbot to understand what specifically we are talking about and asking\n",
    "- Entity Extraction and Intent classification are ML problems, so we need train data to train the models\n",
    "\n",
    "##### Rasa NLU train data\n",
    "- train data should contain example messages which we would like our chatbot to learn from\n",
    "    - the corresponding intents and what entities included in each sentence and where in a sentence they can be found\n",
    "    \n",
    "##### creating data\n",
    "- mkdir data\n",
    "- cd data\n",
    "- echo 'nlu_data' > data.json\n",
    "- there are two different ways of how we can create training examples for NLU models\n",
    "    - one way is to directly write them into this data file\n",
    "    \n",
    "{\n",
    "  \"rasa_nlu_data\":{\n",
    "    \"common_examples\":[\n",
    "    {\n",
    "       \"text\":\"Hello\",\n",
    "       \"intent\":\"greet\",\n",
    "       \"entities\":[]\n",
    "    },\n",
    "    {\n",
    "       \"text\":\"goodbye\",\n",
    "       \"intent\":\"goodbye\",\n",
    "       \"entities\":[]\n",
    "    }\n",
    "      ]\n",
    "  }\n",
    "}\n",
    "\n",
    "    - save this file\n",
    "    - another way :\n",
    "        - in the data folder, launch rasa-nlu-trainer\n",
    "        - here we add new example\n",
    "        - add text\n",
    "            - What's the weather in Berlin at the moment?\n",
    "        - now highlight berlin and add as entity,\n",
    "            - give entity name as location\n",
    "        - Now if we open data.json, we can see entites populated \n",
    "            - also we have start and end to show where entity is present\n",
    "\n",
    "#### Amount of training data\n",
    "- now we have three examples of training \n",
    "- we need more examples for each of these intents\n",
    "    - examples should be different and diverse\n",
    "- now add data from data.json from github to our data.json\n",
    "    - here we have some more examples of greeting, goodbye and asking for weather\n",
    "- reload nlu_trainer\n",
    "- now we will have close to 40 examples in total, which is also less\n",
    "\n",
    "#### Before training\n",
    "- we need to create a configuration file\n",
    "- go out of data folder,\n",
    "    - echo 'config' > config_spacy.json\n",
    "- config file is imp as it provides some params to be used\n",
    "    - 1. pipeline : this will specify what featurizers,feature extractors are going to be used to crunch text messages and extract neccesary info in RASA NLU\n",
    "        - Rasa NLU has two main pipelines pre-built\n",
    "            - a. MIDI based\n",
    "            - b. Sklearn based\n",
    "    - 2. path : dir where we will keep model after training\n",
    "    - 3. data : data file location\n",
    "    \n",
    "- config_spacy.json\n",
    "{\n",
    "  \"pipeline\":\"spacy_sklearn\",\n",
    "  \"path\":\"./models/nlu\",\n",
    "  \"data\":\"./data/data.json\"\n",
    "}\n",
    "\n",
    "#### model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.training_data import load_data\n",
    "from rasa_nlu import config\n",
    "from rasa_nlu.model import Trainer\n",
    "from rasa_nlu.model import Metadata, Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nlu(data, configs, model_dir):\n",
    "    training_data = load_data(data)\n",
    "    # we will have to provide a config in trainer,\n",
    "        # this we wil do using RasaNLUConfig method\n",
    "    trainer = Trainer(config.load(configs))\n",
    "    trainer.train(training_data)\n",
    "    # model dir is where our model is saved\n",
    "    model_directory = trainer.persist( model_dir, fixed_model_name= \"weathernlu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "train_nlu(\"./data/data.json\", \"config_spacy.json\", \"./models/nlu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to check if model is persisted look for models folder\n",
    "- Metadata and Intepreter class is required to load the model and get ready to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_nlu():\n",
    "    # now we load our model\n",
    "    intepreter = Interpreter.load(\"./models/nlu/default/weathernlu\")\n",
    "    pprint(intepreter.parse(u\"I am planning my holiday to Lithuania. I wonder what is the weather out there.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rasa_nlu.components:Added 'nlp_spacy' to component cache. Key 'nlp_spacy-en'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"intent\": {\n",
      "    \"name\": \"inform\",\n",
      "    \"confidence\": 0.843375422673486\n",
      "  },\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"start\": 28,\n",
      "      \"end\": 37,\n",
      "      \"value\": \"lithuania\",\n",
      "      \"entity\": \"location\",\n",
      "      \"confidence\": 0.919550976223837,\n",
      "      \"extractor\": \"ner_crf\"\n",
      "    }\n",
      "  ],\n",
      "  \"intent_ranking\": [\n",
      "    {\n",
      "      \"name\": \"inform\",\n",
      "      \"confidence\": 0.843375422673486\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"greet\",\n",
      "      \"confidence\": 0.07873710059777826\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"goodbye\",\n",
      "      \"confidence\": 0.07788747672873546\n",
      "    }\n",
      "  ],\n",
      "  \"text\": \"I am planning my holiday to Lithuania. I wonder what is the weather out there.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "run_nlu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RASA NLU tells what intent is the text\n",
    "- also it tells the confidence score for all other intents that we have\n",
    "\n",
    "### Dialogue Management model\n",
    "- Dialogue management model will predict what action or resonse the chatbot should make based on the state of the conversation\n",
    "    - actions can be simple API calls or text responses or retriving data from DB\n",
    "- Q. Why to we need ML for that ?\n",
    "    - Now if we ask for weather without giving our location, we want our chatbot to ask for what location we are based\n",
    "        - In practice these type of conversations are hard coded in form of flow charts.\n",
    "        - code wise we can image this as bunch of if else statements\n",
    "        - This means a developer has to create a bunch of most possible happy paths from starting the conversation to the end goal.\n",
    "        - now with every intent and entity we add, the flowchart becomes complex and difficult to monitor all possible paths that user can make to get the answers they want\n",
    "    - In Rasa, we have a ML model which we can train and it will make a prediction of what the bot should do next based on\n",
    "        - a. the context\n",
    "        - b. and the state of the conversation\n",
    "        - as a result, the conversational flow is way more natural and we have a better user experience\n",
    "        \n",
    "### Building dialogue management model\n",
    "- create a domain file for our chatbot\n",
    "- weather_domain.yml\n",
    "    - this will be a yaml file\n",
    "- domain describes an enivronment\n",
    "- domain consists of 5 key parts\n",
    "    - 1. list of slots \n",
    "        - slots are like placeholders that would help chatbot to keep track of context of the conversation\n",
    "            - ex. we are asking weather in specific location\n",
    "                - so chatbot should keep track of the location that we are asking. And we do not remind the chatbot of what location we were speaking initially. So it can keep track of the location in further \n",
    "                - also chatbot is going to make an api call to get the weather information\n",
    "        - so we will create a slot called location, also we will need to tell what data type this slot is going to have\n",
    "            - datatype is imp as diff data types of the slots are going to have effect on how dialogue management model is going to make predictions\n",
    "            - for some datatypes value will be imp in predictions\n",
    "            - for some, whether the slot is populated or not is going to have impact on the prediction made\n",
    "        - in our case, location type will be text\n",
    "        - here we will have only one slot for our example\n",
    "                \n",
    "    - 2. intents\n",
    "        - these are same intents that we had in NLU model\n",
    "            - we had three\n",
    "                 - greet\n",
    "                 - goodbye\n",
    "                 - inform\n",
    "    - 3. entities\n",
    "        - list of entities that chatbot should be aware of and ready of get from user\n",
    "            - we had only one\n",
    "                - location \n",
    "            - here we have an entity called location as well as slot. So NLU model will extract the location name as an entity and will set this value as a slot.\n",
    "                - thats how this value is going to be saved and kept throughout the conversation\n",
    "    - 4. list of templates\n",
    "        - are like text responses that chatbot should send back to the user once specific actions are being predicted\n",
    "        - so we will initialize an action which should be executed when my \n",
    "        - corresponding to an actiong we will write the text message that we want to reply\n",
    "            - ex. utter_greet:\n",
    "                - 'hello ! how can I help ?'\n",
    "            - utter_goodbye:\n",
    "                - 'Talk to you later'\n",
    "                - 'Bye Bye :(' \n",
    "        - we provided more diversity so we added on more possible answer, our chatbot will randomize a little to which answer it will use\n",
    "            - utter_ask_location:\n",
    "                - 'In what location?'\n",
    "    - 5. list of actions\n",
    "        - actions that my chatbot should be ready to execute when they are predicted\n",
    "        - we already have 3 actions create in template\n",
    "            - utter_greet\n",
    "            - utter_goodbye\n",
    "            - utter_ask_location\n",
    "        - note : for returning weather data, we will have a custom action and use python code\n",
    "    - all these 5 are imp as they will be used in dialogue management model to make predictions by RASA Core Dialogue Management model\n",
    "        - RCDM model will make prediction on what actions should me executed next on the slots that are currently populated based on \n",
    "            - a. intents and entities returned by Rasa NLU model ie. what a user spoke about\n",
    "            - b. what actions were performed previously ie. what is the state of the conversation at the moment\n",
    "          \n",
    "### Custom Actions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# future enables new language features which may not compatible with\n",
    "    # current interpreter\n",
    "# So that code will work with older version of python\n",
    "\n",
    "from __future__ import absolute_import, division, unicode_literals\n",
    "\n",
    "from rasa_core.actions.action import Action\n",
    "from rasa_core.events import SlotSet\n",
    "\n",
    "# \n",
    "class ActionWeather(Action):\n",
    "    \n",
    "    # create name of the action\n",
    "    def name(self):\n",
    "        return \"action_weather\"\n",
    "    \n",
    "    # here all the action will happen\n",
    "    # apixu\n",
    "    def run(self, dispatcher, tracker, domain):\n",
    "        from apixu.client import ApixuClient\n",
    "        api_key = \"3564bf1fbe6d44d0b4c93136190906\"\n",
    "        # authentication\n",
    "        client = ApixuClient(api_key)\n",
    "        # remember we have a slot which keeps location info\n",
    "            # through out the conversation\n",
    "        # from tracker get value of a particular slot    \n",
    "        loc = tracker.get_slot(\"location\")\n",
    "        \n",
    "        # response is going to be a dictionary\n",
    "            # having lot of details\n",
    "        current = client.getCurrentWeather(q=loc)\n",
    "        \n",
    "        # now we will parse the response\n",
    "        country = current['location']['country']\n",
    "        city = current['location']['name']\n",
    "        condition = current['current']['condition']['text']\n",
    "        temp_c = current['current']['temp_c']\n",
    "        humidity = current['current']['humidity']\n",
    "        wind_mph = current['current']['wind_mph']\n",
    "        \n",
    "        # now we will create response message\n",
    "        response = \"\"\"\n",
    "            It is currently {} in {} at the moment. The temperature is {} degrees,\n",
    "             The Humidity is {}% and the wind speed is {} mph.\"\"\".format(condition,\n",
    "                                                                         city, \n",
    "                                                                         temp_c,\n",
    "                                                                         humidity,\n",
    "                                                                         wind_mph);\n",
    "        # dispatcher will send out the response\n",
    "        dispatcher.utter_message(response)\n",
    "        \n",
    "        # lastly we will return current slot value\n",
    "        return [SlotSet('location',loc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we have to include this to our domain\n",
    "    - under actions\n",
    "        - actions.ActionWeather\n",
    "    - note : here Actionweather class is in file actions.py\n",
    "    - in jupyter add __main__.ActionWeather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'domain_yml_data' (str) to file 'weather_domain.yml'.\n"
     ]
    }
   ],
   "source": [
    "domain_yml_data = \"\"\"\n",
    "slots:\n",
    "  location:\n",
    "    type: text\n",
    "\n",
    "\n",
    "intents:\n",
    " - greet\n",
    " - goodbye\n",
    " - inform\n",
    "\n",
    "\n",
    "entities:\n",
    " - location\n",
    "\n",
    "templates:\n",
    "  utter_greet:\n",
    "    - 'Hello! How can I help?'\n",
    "  utter_goodbye:\n",
    "    - 'Talk to you later.'\n",
    "    - 'Bye bye :('\n",
    "  utter_ask_location:\n",
    "    - 'In what location?'\n",
    "\n",
    "\n",
    "actions:\n",
    " - utter_greet\n",
    " - utter_goodbye\n",
    " - utter_ask_location\n",
    " - __main__.ActionWeather\n",
    "\"\"\" \n",
    " \n",
    "%store domain_yml_data > weather_domain.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "- dialogue management model is trained on actual conversations that users have"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
