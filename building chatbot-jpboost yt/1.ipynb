{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "- Rasa core and Rasa NLU combined is called RASA STACK\n",
    "- We will build a weather reporting chatbot\n",
    "    - this will do Entity Extraction and Intent classification task\n",
    "    \n",
    "### Setup\n",
    "1. pip install -r requirements.txt\n",
    "2. language model ( en ) \n",
    "    - lanuage model is used to parse incoming text messages and extract necessary information\n",
    "3. Rasa NLU trainer\n",
    "    - this makes generating training data lot easier\n",
    "    - this is a UI, and js based application, so we will need npm and nodejs\n",
    "    - download node js with npm and add to path\n",
    "    - then run\n",
    "        - npm i -g rasa-nlu-trainer\n",
    "    - this will install rasa-nlu trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import logging, io, json, warnings\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def pprint(o):\n",
    "    # small helper to make dict dumps a bit prettier\n",
    "    print(json.dumps(o, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 92.1MB/s ta 0:00:01   2% |▊                               | 860kB 5.1MB/s eta 0:00:08    4% |█▎                              | 1.5MB 30.3MB/s eta 0:00:02    4% |█▌                              | 1.7MB 33.4MB/s eta 0:00:02    4% |█▋                              | 1.9MB 2.6MB/s eta 0:00:14/s eta 0:00:01    17% |█████▌                          | 6.4MB 6.9MB/s eta 0:00:05    18% |██████                          | 7.0MB 11.3MB/s eta 0:00:03    21% |██████▉                         | 8.0MB 1.4MB/s eta 0:00:21    23% |███████▌                        | 8.8MB 1.7MB/s eta 0:00:17    29% |█████████▌                      | 11.1MB 5.1MB/s eta 0:00:06    33% |██████████▊                     | 12.5MB 8.9MB/s eta 0:00:03    39% |████████████▌                   | 14.6MB 6.7MB/s eta 0:00:04    39% |████████████▋                   | 14.8MB 2.1MB/s eta 0:00:11    43% |██████████████                  | 16.4MB 3.5MB/s eta 0:00:06    44% |██████████████▏                 | 16.5MB 2.1MB/s eta 0:00:10    55% |█████████████████▊              | 20.7MB 8.0MB/s eta 0:00:03    57% |██████████████████▎             | 21.3MB 6.0MB/s eta 0:00:03    61% |███████████████████▌            | 22.8MB 4.4MB/s eta 0:00:04    62% |████████████████████            | 23.3MB 5.1MB/s eta 0:00:03    64% |████████████████████▋           | 24.1MB 3.0MB/s eta 0:00:05    71% |██████████████████████▉         | 26.7MB 3.1MB/s eta 0:00:04    74% |███████████████████████▊        | 27.8MB 7.7MB/s eta 0:00:02    77% |████████████████████████▋       | 28.8MB 1.8MB/s eta 0:00:05    77% |█████████████████████████       | 29.1MB 5.0MB/s eta 0:00:02    80% |█████████████████████████▋      | 29.9MB 7.3MB/s eta 0:00:02    89% |████████████████████████████▋   | 33.4MB 3.4MB/s eta 0:00:02    91% |█████████████████████████████▎  | 34.2MB 1.2MB/s eta 0:00:03    91% |█████████████████████████████▍  | 34.3MB 2.0MB/s eta 0:00:02\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/vivek/anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /home/vivek/anaconda3/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. install language model \n",
    "\n",
    "import sys\n",
    "python = sys.executable\n",
    "\n",
    "# this will download english spacy model\n",
    "    # it will install it and reference it to abbreviation en\n",
    "!{python} -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "- we have to teach a chatbot how to understand human unstructured language so that bot will understand what we are saying\n",
    "    - So we will train NLU model which will take unstructured text messages and will return structured data in the form of intents and entities which our bot will understand\n",
    "- Intent\n",
    "    - what the message is about\n",
    "- Entity\n",
    "    - informations like location names, dates etc\n",
    "    - this helps chatbot to understand what specifically we are talking about and asking\n",
    "- Entity Extraction and Intent classification are ML problems, so we need train data to train the models\n",
    "\n",
    "##### Rasa NLU train data\n",
    "- train data should contain example messages which we would like our chatbot to learn from\n",
    "    - the corresponding intents and what entities included in each sentence and where in a sentence they can be found\n",
    "    \n",
    "##### creating data\n",
    "- mkdir data\n",
    "- cd data\n",
    "- echo 'nlu_data' > data.json\n",
    "- there are two different ways of how we can create training examples for NLU models\n",
    "    - one way is to directly write them into this data file\n",
    "    \n",
    "{\n",
    "  \"rasa_nlu_data\":{\n",
    "    \"common_examples\":[\n",
    "    {\n",
    "       \"text\":\"Hello\",\n",
    "       \"intent\":\"greet\",\n",
    "       \"entities\":[]\n",
    "    },\n",
    "    {\n",
    "       \"text\":\"goodbye\",\n",
    "       \"intent\":\"goodbye\",\n",
    "       \"entities\":[]\n",
    "    }\n",
    "      ]\n",
    "  }\n",
    "}\n",
    "\n",
    "    - save this file\n",
    "    - another way :\n",
    "        - in the data folder, launch rasa-nlu-trainer\n",
    "        - here we add new example\n",
    "        - add text\n",
    "            - What's the weather in Berlin at the moment?\n",
    "        - now highlight berlin and add as entity,\n",
    "            - give entity name as location\n",
    "        - Now if we open data.json, we can see entites populated \n",
    "            - also we have start and end to show where entity is present\n",
    "\n",
    "#### Amount of training data\n",
    "- now we have three examples of training \n",
    "- we need more examples for each of these intents\n",
    "    - examples should be different and diverse\n",
    "- now add data from data.json from github to our data.json\n",
    "    - here we have some more examples of greeting, goodbye and asking for weather\n",
    "- reload nlu_trainer\n",
    "- now we will have close to 40 examples in total, which is also less\n",
    "\n",
    "#### Before training\n",
    "- we need to create a configuration file\n",
    "- go out of data folder,\n",
    "    - echo 'config' > config_spacy.json\n",
    "- config file is imp as it provides some params to be used\n",
    "    - 1. pipeline : this will specify what featurizers,feature extractors are going to be used to crunch text messages and extract neccesary info in RASA NLU\n",
    "        - Rasa NLU has two main pipelines pre-built\n",
    "            - a. MIDI based\n",
    "            - b. Sklearn based\n",
    "    - 2. path : dir where we will keep model after training\n",
    "    - 3. data : data file location\n",
    "    \n",
    "- config_spacy.json\n",
    "{\n",
    "  \"pipeline\":\"spacy_sklearn\",\n",
    "  \"path\":\"./models/nlu\",\n",
    "  \"data\":\"./data/data.json\"\n",
    "}\n",
    "\n",
    "#### model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.training_data import load_data\n",
    "from rasa_nlu import config\n",
    "from rasa_nlu.model import Trainer\n",
    "from rasa_nlu.model import Metadata, Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nlu(data, configs, model_dir):\n",
    "    training_data = load_data(data)\n",
    "    # we will have to provide a config in trainer,\n",
    "        # this we wil do using RasaNLUConfig method\n",
    "    trainer = Trainer(config.load(configs))\n",
    "    trainer.train(training_data)\n",
    "    # model dir is where our model is saved\n",
    "    model_directory = trainer.persist( model_dir, fixed_model_name= \"weathernlu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vivek/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.1s finished\n"
     ]
    }
   ],
   "source": [
    "train_nlu(\"./data/data.json\", \"config_spacy.json\", \"./models/nlu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to check if model is persisted look for models folder\n",
    "- Metadata and Intepreter class is required to load the model and get ready to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_nlu():\n",
    "    # now we load our model\n",
    "    intepreter = Interpreter.load(\"./models/nlu/default/weathernlu\")\n",
    "    pprint(intepreter.parse(u\"I am planning my holiday to Lithuania. I wonder what is the weather out there.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rasa_nlu.components:Added 'nlp_spacy' to component cache. Key 'nlp_spacy-en'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"intent\": {\n",
      "    \"name\": \"inform\",\n",
      "    \"confidence\": 0.843375422673486\n",
      "  },\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"start\": 28,\n",
      "      \"end\": 37,\n",
      "      \"value\": \"lithuania\",\n",
      "      \"entity\": \"location\",\n",
      "      \"confidence\": 0.919550976223837,\n",
      "      \"extractor\": \"ner_crf\"\n",
      "    }\n",
      "  ],\n",
      "  \"intent_ranking\": [\n",
      "    {\n",
      "      \"name\": \"inform\",\n",
      "      \"confidence\": 0.843375422673486\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"greet\",\n",
      "      \"confidence\": 0.07873710059777826\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"goodbye\",\n",
      "      \"confidence\": 0.07788747672873546\n",
      "    }\n",
      "  ],\n",
      "  \"text\": \"I am planning my holiday to Lithuania. I wonder what is the weather out there.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "run_nlu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RASA NLU tells what intent is the text\n",
    "- also it tells the confidence score for all other intents that we have\n",
    "\n",
    "### Dialogue Management model\n",
    "- Dialogue management model will predict what action or resonse the chatbot should make based on the state of the conversation\n",
    "    - actions can be simple API calls or text responses or retriving data from DB\n",
    "- Q. Why to we need ML for that ?\n",
    "    - Now if we ask for weather without giving our location, we want our chatbot to ask for what location we are based\n",
    "        - In practice these type of conversations are hard coded in form of flow charts.\n",
    "        - code wise we can image this as bunch of if else statements\n",
    "        - This means a developer has to create a bunch of most possible happy paths from starting the conversation to the end goal.\n",
    "        - now with every intent and entity we add, the flowchart becomes complex and difficult to monitor all possible paths that user can make to get the answers they want\n",
    "    - In Rasa, we have a ML model which we can train and it will make a prediction of what the bot should do next based on\n",
    "        - a. the context\n",
    "        - b. and the state of the conversation\n",
    "        - as a result, the conversational flow is way more natural and we have a better user experience\n",
    "        \n",
    "### Building dialogue management model\n",
    "- create a domain file for our chatbot\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
