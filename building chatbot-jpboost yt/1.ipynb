{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intro\n",
    "- Rasa core and Rasa NLU combined is called RASA STACK\n",
    "- We will build a weather reporting chatbot\n",
    "    - this will do Entity Extraction and Intent classification task\n",
    "    \n",
    "### Setup\n",
    "1. pip install -r requirements.txt\n",
    "2. language model ( en ) \n",
    "    - lanuage model is used to parse incoming text messages and extract necessary information\n",
    "3. Rasa NLU trainer\n",
    "    - this makes generating training data lot easier\n",
    "    - this is a UI, and js based application, so we will need npm and nodejs\n",
    "    - download node js with npm and add to path\n",
    "    - then run\n",
    "        - npm i -g rasa-nlu-trainer\n",
    "    - this will install rasa-nlu trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import logging, io, json, warnings\n",
    "logging.basicConfig(level=\"INFO\")\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def pprint(o):\n",
    "    # small helper to make dict dumps a bit prettier\n",
    "    print(json.dumps(o, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n",
      "\u001b[K    100% |████████████████████████████████| 37.4MB 92.1MB/s ta 0:00:01   2% |▊                               | 860kB 5.1MB/s eta 0:00:08    4% |█▎                              | 1.5MB 30.3MB/s eta 0:00:02    4% |█▌                              | 1.7MB 33.4MB/s eta 0:00:02    4% |█▋                              | 1.9MB 2.6MB/s eta 0:00:14/s eta 0:00:01    17% |█████▌                          | 6.4MB 6.9MB/s eta 0:00:05    18% |██████                          | 7.0MB 11.3MB/s eta 0:00:03    21% |██████▉                         | 8.0MB 1.4MB/s eta 0:00:21    23% |███████▌                        | 8.8MB 1.7MB/s eta 0:00:17    29% |█████████▌                      | 11.1MB 5.1MB/s eta 0:00:06    33% |██████████▊                     | 12.5MB 8.9MB/s eta 0:00:03    39% |████████████▌                   | 14.6MB 6.7MB/s eta 0:00:04    39% |████████████▋                   | 14.8MB 2.1MB/s eta 0:00:11    43% |██████████████                  | 16.4MB 3.5MB/s eta 0:00:06    44% |██████████████▏                 | 16.5MB 2.1MB/s eta 0:00:10    55% |█████████████████▊              | 20.7MB 8.0MB/s eta 0:00:03    57% |██████████████████▎             | 21.3MB 6.0MB/s eta 0:00:03    61% |███████████████████▌            | 22.8MB 4.4MB/s eta 0:00:04    62% |████████████████████            | 23.3MB 5.1MB/s eta 0:00:03    64% |████████████████████▋           | 24.1MB 3.0MB/s eta 0:00:05    71% |██████████████████████▉         | 26.7MB 3.1MB/s eta 0:00:04    74% |███████████████████████▊        | 27.8MB 7.7MB/s eta 0:00:02    77% |████████████████████████▋       | 28.8MB 1.8MB/s eta 0:00:05    77% |█████████████████████████       | 29.1MB 5.0MB/s eta 0:00:02    80% |█████████████████████████▋      | 29.9MB 7.3MB/s eta 0:00:02    89% |████████████████████████████▋   | 33.4MB 3.4MB/s eta 0:00:02    91% |█████████████████████████████▎  | 34.2MB 1.2MB/s eta 0:00:03    91% |█████████████████████████████▍  | 34.3MB 2.0MB/s eta 0:00:02\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "  Running setup.py install for en-core-web-sm ... \u001b[?25ldone\n",
      "\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n",
      "\u001b[33mYou are using pip version 9.0.1, however version 19.1.1 is available.\n",
      "You should consider upgrading via the 'pip install --upgrade pip' command.\u001b[0m\n",
      "\n",
      "\u001b[93m    Linking successful\u001b[0m\n",
      "    /home/vivek/anaconda3/lib/python3.6/site-packages/en_core_web_sm -->\n",
      "    /home/vivek/anaconda3/lib/python3.6/site-packages/spacy/data/en\n",
      "\n",
      "    You can now load the model via spacy.load('en')\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. install language model \n",
    "\n",
    "import sys\n",
    "python = sys.executable\n",
    "\n",
    "# this will download english spacy model\n",
    "    # it will install it and reference it to abbreviation en\n",
    "!{python} -m spacy download en"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training \n",
    "- we have to teach a chatbot how to understand human unstructured language so that bot will understand what we are saying\n",
    "    - So we will train NLU model which will take unstructured text messages and will return structured data in the form of intents and entities which our bot will understand\n",
    "- Intent\n",
    "    - what the message is about\n",
    "- Entity\n",
    "    - informations like location names, dates etc\n",
    "    - this helps chatbot to understand what specifically we are talking about and asking\n",
    "- Entity Extraction and Intent classification are ML problems, so we need train data to train the models\n",
    "\n",
    "##### Rasa NLU train data\n",
    "- train data should contain example messages which we would like our chatbot to learn from\n",
    "    - the corresponding intents and what entities included in each sentence and where in a sentence they can be found\n",
    "    \n",
    "##### creating data\n",
    "- mkdir data\n",
    "- cd data\n",
    "- echo 'nlu_data' > data.json\n",
    "- there are two different ways of how we can create training examples for NLU models\n",
    "    - one way is to directly write them into this data file\n",
    "    \n",
    "{\n",
    "  \"rasa_nlu_data\":{\n",
    "    \"common_examples\":[\n",
    "    {\n",
    "       \"text\":\"Hello\",\n",
    "       \"intent\":\"greet\",\n",
    "       \"entities\":[]\n",
    "    },\n",
    "    {\n",
    "       \"text\":\"goodbye\",\n",
    "       \"intent\":\"goodbye\",\n",
    "       \"entities\":[]\n",
    "    }\n",
    "      ]\n",
    "  }\n",
    "}\n",
    "\n",
    "    - save this file\n",
    "    - another way :\n",
    "        - in the data folder, launch rasa-nlu-trainer\n",
    "        - here we add new example\n",
    "        - add text\n",
    "            - What's the weather in Berlin at the moment?\n",
    "        - now highlight berlin and add as entity,\n",
    "            - give entity name as location\n",
    "        - Now if we open data.json, we can see entites populated \n",
    "            - also we have start and end to show where entity is present\n",
    "\n",
    "#### Amount of training data\n",
    "- now we have three examples of training \n",
    "- we need more examples for each of these intents\n",
    "    - examples should be different and diverse\n",
    "- now add data from data.json from github to our data.json\n",
    "    - here we have some more examples of greeting, goodbye and asking for weather\n",
    "- reload nlu_trainer\n",
    "- now we will have close to 40 examples in total, which is also less\n",
    "\n",
    "#### Before training\n",
    "- we need to create a configuration file\n",
    "- go out of data folder,\n",
    "    - echo 'config' > config_spacy.json\n",
    "- config file is imp as it provides some params to be used\n",
    "    - 1. pipeline : this will specify what featurizers,feature extractors are going to be used to crunch text messages and extract neccesary info in RASA NLU\n",
    "        - Rasa NLU has two main pipelines pre-built\n",
    "            - a. MIDI based\n",
    "            - b. Sklearn based\n",
    "    - 2. path : dir where we will keep model after training\n",
    "    - 3. data : data file location\n",
    "    \n",
    "- config_spacy.json\n",
    "{\n",
    "  \"pipeline\":\"spacy_sklearn\",\n",
    "  \"path\":\"./models/nlu\",\n",
    "  \"data\":\"./data/data.json\"\n",
    "}\n",
    "\n",
    "#### model creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rasa_nlu.training_data import load_data\n",
    "from rasa_nlu import config\n",
    "from rasa_nlu.model import Trainer\n",
    "from rasa_nlu.model import Metadata, Interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_nlu(data, configs, model_dir):\n",
    "    training_data = load_data(data)\n",
    "    # we will have to provide a config in trainer,\n",
    "        # this we wil do using RasaNLUConfig method\n",
    "    trainer = Trainer(config.load(configs))\n",
    "    trainer.train(training_data)\n",
    "    # model dir is where our model is saved\n",
    "    model_directory = trainer.persist( model_dir, fixed_model_name= \"weathernlu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rasa_nlu.training_data.loading:Training data format of ./data/data.json is rasa_nlu\n",
      "INFO:rasa_nlu.training_data.training_data:Training data stats: \n",
      "\t- intent examples: 29 (3 distinct intents)\n",
      "\t- Found intents: 'greet', 'inform', 'goodbye'\n",
      "\t- entity examples: 13 (1 distinct entities)\n",
      "\t- found entities: 'location'\n",
      "\n",
      "INFO:rasa_nlu.utils.spacy_utils:Trying to load spacy model with name 'en'\n",
      "INFO:rasa_nlu.components:Added 'nlp_spacy' to component cache. Key 'nlp_spacy-en'.\n",
      "INFO:rasa_nlu.model:Starting to train component nlp_spacy\n",
      "INFO:rasa_nlu.model:Finished training component.\n",
      "INFO:rasa_nlu.model:Starting to train component tokenizer_spacy\n",
      "INFO:rasa_nlu.model:Finished training component.\n",
      "INFO:rasa_nlu.model:Starting to train component intent_featurizer_spacy\n",
      "INFO:rasa_nlu.model:Finished training component.\n",
      "INFO:rasa_nlu.model:Starting to train component intent_entity_featurizer_regex\n",
      "INFO:rasa_nlu.model:Finished training component.\n",
      "INFO:rasa_nlu.model:Starting to train component ner_crf\n",
      "INFO:rasa_nlu.model:Finished training component.\n",
      "INFO:rasa_nlu.model:Starting to train component ner_synonyms\n",
      "INFO:rasa_nlu.model:Finished training component.\n",
      "INFO:rasa_nlu.model:Starting to train component intent_classifier_sklearn\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  12 out of  12 | elapsed:    0.1s finished\n",
      "INFO:rasa_nlu.model:Finished training component.\n",
      "INFO:rasa_nlu.model:Successfully saved model into '/home/vivek/Desktop/jupyter files/RASA-Chatbot/building chatbot-jpboost yt/models/nlu/default/weathernlu'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 6 candidates, totalling 12 fits\n"
     ]
    }
   ],
   "source": [
    "train_nlu(\"./data/data.json\", \"config_spacy.json\", \"./models/nlu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- to check if model is persisted look for models folder\n",
    "- Metadata and Intepreter class is required to load the model and get ready to use it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_nlu():\n",
    "    # now we load our model\n",
    "    intepreter = Interpreter.load(\"./models/nlu/default/weathernlu\")\n",
    "    pprint(intepreter.parse(u\"I am planning my holiday to Lithuania. I wonder what is the weather out there.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:rasa_nlu.components:Added 'nlp_spacy' to component cache. Key 'nlp_spacy-en'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"intent\": {\n",
      "    \"name\": \"inform\",\n",
      "    \"confidence\": 0.8509291192465037\n",
      "  },\n",
      "  \"entities\": [\n",
      "    {\n",
      "      \"start\": 28,\n",
      "      \"end\": 37,\n",
      "      \"value\": \"lithuania\",\n",
      "      \"entity\": \"location\",\n",
      "      \"confidence\": 0.919550976223837,\n",
      "      \"extractor\": \"ner_crf\"\n",
      "    }\n",
      "  ],\n",
      "  \"intent_ranking\": [\n",
      "    {\n",
      "      \"name\": \"inform\",\n",
      "      \"confidence\": 0.8509291192465037\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"goodbye\",\n",
      "      \"confidence\": 0.07978649851713138\n",
      "    },\n",
      "    {\n",
      "      \"name\": \"greet\",\n",
      "      \"confidence\": 0.06928438223636452\n",
      "    }\n",
      "  ],\n",
      "  \"text\": \"I am planning my holiday to Lithuania. I wonder what is the weather out there.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "run_nlu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- RASA NLU tells what intent is the text\n",
    "- also it tells the confidence score for all other intents that we have\n",
    "\n",
    "### Dialogue Management model\n",
    "- Dialogue management model will predict what action or resonse the chatbot should make based on the state of the conversation\n",
    "    - actions can be simple API calls or text responses or retriving data from DB\n",
    "- Q. Why to we need ML for that ?\n",
    "    - Now if we ask for weather without giving our location, we want our chatbot to ask for what location we are based\n",
    "        - In practice these type of conversations are hard coded in form of flow charts.\n",
    "        - code wise we can image this as bunch of if else statements\n",
    "        - This means a developer has to create a bunch of most possible happy paths from starting the conversation to the end goal.\n",
    "        - now with every intent and entity we add, the flowchart becomes complex and difficult to monitor all possible paths that user can make to get the answers they want\n",
    "    - In Rasa, we have a ML model which we can train and it will make a prediction of what the bot should do next based on\n",
    "        - a. the context\n",
    "        - b. and the state of the conversation\n",
    "        - as a result, the conversational flow is way more natural and we have a better user experience\n",
    "        \n",
    "### Building dialogue management model\n",
    "- create a domain file for our chatbot\n",
    "- weather_domain.yml\n",
    "    - this will be a yaml file\n",
    "- domain describes an enivronment\n",
    "- domain consists of 5 key parts\n",
    "    - 1. list of slots \n",
    "        - slots are like placeholders that would help chatbot to keep track of context of the conversation\n",
    "            - ex. we are asking weather in specific location\n",
    "                - so chatbot should keep track of the location that we are asking. And we do not remind the chatbot of what location we were speaking initially. So it can keep track of the location in further \n",
    "                - also chatbot is going to make an api call to get the weather information\n",
    "        - so we will create a slot called location, also we will need to tell what data type this slot is going to have\n",
    "            - datatype is imp as diff data types of the slots are going to have effect on how dialogue management model is going to make predictions\n",
    "            - for some datatypes value will be imp in predictions\n",
    "            - for some, whether the slot is populated or not is going to have impact on the prediction made\n",
    "        - in our case, location type will be text\n",
    "        - here we will have only one slot for our example\n",
    "                \n",
    "    - 2. intents\n",
    "        - these are same intents that we had in NLU model\n",
    "            - we had three\n",
    "                 - greet\n",
    "                 - goodbye\n",
    "                 - inform\n",
    "    - 3. entities\n",
    "        - list of entities that chatbot should be aware of and ready of get from user\n",
    "            - we had only one\n",
    "                - location \n",
    "            - here we have an entity called location as well as slot. So NLU model will extract the location name as an entity and will set this value as a slot.\n",
    "                - thats how this value is going to be saved and kept throughout the conversation\n",
    "    - 4. list of templates\n",
    "        - are like text responses that chatbot should send back to the user once specific actions are being predicted\n",
    "        - so we will initialize an action which should be executed when my \n",
    "        - corresponding to an actiong we will write the text message that we want to reply\n",
    "            - ex. utter_greet:\n",
    "                - 'hello ! how can I help ?'\n",
    "            - utter_goodbye:\n",
    "                - 'Talk to you later'\n",
    "                - 'Bye Bye :(' \n",
    "        - we provided more diversity so we added on more possible answer, our chatbot will randomize a little to which answer it will use\n",
    "            - utter_ask_location:\n",
    "                - 'In what location?'\n",
    "    - 5. list of actions\n",
    "        - actions that my chatbot should be ready to execute when they are predicted\n",
    "        - we already have 3 actions create in template\n",
    "            - utter_greet\n",
    "            - utter_goodbye\n",
    "            - utter_ask_location\n",
    "        - note : for returning weather data, we will have a custom action and use python code\n",
    "    - all these 5 are imp as they will be used in dialogue management model to make predictions by RASA Core Dialogue Management model\n",
    "        - RCDM model will make prediction on what actions should me executed next on the slots that are currently populated based on \n",
    "            - a. intents and entities returned by Rasa NLU model ie. what a user spoke about\n",
    "            - b. what actions were performed previously ie. what is the state of the conversation at the moment\n",
    "          \n",
    "### Custom Actions  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# future enables new language features which may not compatible with\n",
    "    # current interpreter\n",
    "# So that code will work with older version of python\n",
    "\n",
    "from __future__ import absolute_import, division, unicode_literals\n",
    "\n",
    "from rasa_core.actions.action import Action\n",
    "from rasa_core.events import SlotSet\n",
    "\n",
    "# \n",
    "class ActionWeather(Action):\n",
    "    \n",
    "    # create name of the action\n",
    "    def name(self):\n",
    "        return \"action_weather\"\n",
    "    \n",
    "    # here all the action will happen\n",
    "    # apixu\n",
    "    def run(self, dispatcher, tracker, domain):\n",
    "        from apixu.client import ApixuClient\n",
    "        api_key = \"3564bf1fbe6d44d0b4c93136190906\"\n",
    "        # authentication\n",
    "        client = ApixuClient(api_key)\n",
    "        # remember we have a slot which keeps location info\n",
    "            # through out the conversation\n",
    "        # from tracker get value of a particular slot    \n",
    "        loc = tracker.get_slot(\"location\")\n",
    "        \n",
    "        # response is going to be a dictionary\n",
    "            # having lot of details\n",
    "        current = client.getCurrentWeather(q=loc)\n",
    "        \n",
    "        # now we will parse the response\n",
    "        country = current['location']['country']\n",
    "        city = current['location']['name']\n",
    "        condition = current['current']['condition']['text']\n",
    "        temp_c = current['current']['temp_c']\n",
    "        humidity = current['current']['humidity']\n",
    "        wind_mph = current['current']['wind_mph']\n",
    "        \n",
    "        # now we will create response message\n",
    "        response = \"\"\"\n",
    "            It is currently {} in {} at the moment. The temperature is {} degrees,\n",
    "             The Humidity is {}% and the wind speed is {} mph.\"\"\".format(condition,\n",
    "                                                                         city, \n",
    "                                                                         temp_c,\n",
    "                                                                         humidity,\n",
    "                                                                         wind_mph);\n",
    "        # dispatcher will send out the response\n",
    "        dispatcher.utter_message(response)\n",
    "        \n",
    "        # lastly we will return current slot value\n",
    "        return [SlotSet('location',loc)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- now we have to include this to our domain\n",
    "    - under actions\n",
    "        - actions.ActionWeather\n",
    "    - note : here Actionweather class is in file actions.py\n",
    "    - in jupyter add __main__.ActionWeather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'domain_yml_data' (str) to file 'weather_domain.yml'.\n"
     ]
    }
   ],
   "source": [
    "domain_yml_data = \"\"\"\n",
    "slots:\n",
    "  location:\n",
    "    type: text\n",
    "\n",
    "\n",
    "intents:\n",
    " - greet\n",
    " - goodbye\n",
    " - inform\n",
    "\n",
    "\n",
    "entities:\n",
    " - location\n",
    "\n",
    "templates:\n",
    "  utter_greet:\n",
    "    - 'Hello! How can I help?'\n",
    "  utter_goodbye:\n",
    "    - 'Talk to you later.'\n",
    "    - 'Bye bye :('\n",
    "  utter_ask_location:\n",
    "    - 'In what location?'\n",
    "\n",
    "\n",
    "actions:\n",
    " - utter_greet\n",
    " - utter_goodbye\n",
    " - utter_ask_location\n",
    "\n",
    "\"\"\" \n",
    " \n",
    "%store domain_yml_data > weather_domain.yml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "- dialogue management model is trained on actual conversations that users have with the bot\n",
    "    - only thing is that these conversations have to be converted into a story format\n",
    "    - story is and actual conversation b/w a user and a chatbot where user inputs are converted into correspoding intents and entities while response of chatbots are expressed as actions which the bot would execute at that specific stage\n",
    "- example of how a real conversation can be written as a story\n",
    "<img src=\"rasa1.JPG\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- but before a question arises is how to get this data in the first place\n",
    "    - One way is to use Rasa core NLU feature called Online Training\n",
    "        - this will not only help with generating data, but also train a dialogue management model in real time\n",
    "\n",
    "#### Generating Stories\n",
    "- go to data folder and create stories.md\n",
    "    - this will be markdown extension\n",
    "- we will create some stateless stories\n",
    "    - stateless stories are conversations where we have one user input and one user response\n",
    "- * intent : conversation that will start with intent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing 'stories' (str) to file 'stories.md'.\n"
     ]
    }
   ],
   "source": [
    "stories = \"\"\"\n",
    "\n",
    "## story 01\n",
    "* greet\n",
    "    - utter_greet\n",
    "\n",
    "## story 02\n",
    "* goodbye\n",
    "    - utter_goodbye\n",
    "    \n",
    "## story 03\n",
    "* inform\n",
    "    - utter_ask_location\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "%store stories > stories.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- we can start online training session\n",
    "\n",
    "### training\n",
    "- Agent : it is going to train the model\n",
    "- KerasPolicy/ Memoization policy : models to be used to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import logging\n",
    "\n",
    "from rasa_core.agent import Agent\n",
    "from rasa_core.policies.keras_policy import KerasPolicy\n",
    "from rasa_core.policies.memoization import MemoizationPolicy\n",
    "\n",
    "logging.basicConfig(level='INFO')\n",
    "\n",
    "training_data_file = 'stories.md'\n",
    "\n",
    "# where to save model once trained\n",
    "model_path = './models/dialogue'\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:rasa_core.agent:Passing a file name to `agent.train(...)` is deprecated. Rather load the data with `data = agent.load_data(file_name)` and pass it to `agent.train(data)`.\n",
      "Processed Story Blocks: 100%|██████████| 3/3 [00:00<00:00, 135.31it/s, # trackers=1]\n",
      "Processed Story Blocks: 100%|██████████| 3/3 [00:00<00:00, 78.35it/s, # trackers=3]\n",
      "Processed Story Blocks: 100%|██████████| 3/3 [00:00<00:00, 72.87it/s, # trackers=12]\n",
      "Processed Story Blocks: 100%|██████████| 3/3 [00:00<00:00, 65.13it/s, # trackers=20]\n",
      "INFO:rasa_core.featurizers:Creating states and action examples from collected trackers (by MaxHistoryTrackerFeaturizer)...\n",
      "Processed trackers: 100%|██████████| 84/84 [00:05<00:00, 12.57it/s, # actions=79]\n",
      "INFO:rasa_core.featurizers:Created 79 action examples.\n",
      "Processed actions: 79it [00:00, 160.19it/s, # examples=79]\n",
      "INFO:rasa_core.policies.memoization:Memorized 79 unique action examples.\n",
      "INFO:rasa_core.featurizers:Creating states and action examples from collected trackers (by MaxHistoryTrackerFeaturizer)...\n",
      "Processed trackers:  29%|██▊       | 24/84 [00:00<00:02, 23.86it/s, # actions=66]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-e33677db10c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m validation_split = 0.2)\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/site-packages/rasa_core/agent.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_trackers, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m         self.policy_ensemble.train(training_trackers, self.domain,\n\u001b[0;32m--> 263\u001b[0;31m                                    **kwargs)\n\u001b[0m\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m     def train_online(self,\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/site-packages/rasa_core/policies/ensemble.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_trackers, domain, **kwargs)\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtraining_trackers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mpolicy\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m                 \u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtraining_trackers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_trackers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_trackers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/site-packages/rasa_core/policies/keras_policy.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, training_trackers, domain, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m         training_data = self.featurize_for_training(training_trackers,\n\u001b[1;32m    139\u001b[0m                                                     \u001b[0mdomain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 140\u001b[0;31m                                                     **kwargs)\n\u001b[0m\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0mshuffled_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffled_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshuffled_X_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/site-packages/rasa_core/policies/policy.py\u001b[0m in \u001b[0;36mfeaturize_for_training\u001b[0;34m(self, training_trackers, domain, **kwargs)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m         training_data = self.featurizer.featurize_trackers(training_trackers,\n\u001b[0;32m---> 79\u001b[0;31m                                                            domain)\n\u001b[0m\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m         \u001b[0mmax_training_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'max_training_samples'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/site-packages/rasa_core/featurizers.py\u001b[0m in \u001b[0;36mfeaturize_trackers\u001b[0;34m(self, trackers, domain)\u001b[0m\n\u001b[1;32m    398\u001b[0m         (trackers_as_states,\n\u001b[1;32m    399\u001b[0m          \u001b[0mtrackers_as_actions\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining_states_and_actions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrackers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 400\u001b[0;31m                                                                  domain)\n\u001b[0m\u001b[1;32m    401\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    402\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrue_lengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_featurize_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrackers_as_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/site-packages/rasa_core/featurizers.py\u001b[0m in \u001b[0;36mtraining_states_and_actions\u001b[0;34m(self, trackers, domain)\u001b[0m\n\u001b[1;32m    620\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    621\u001b[0m                         pbar.set_postfix({\"# actions\": \"{:d}\".format(\n\u001b[0;32m--> 622\u001b[0;31m                             len(trackers_as_actions))})\n\u001b[0m\u001b[1;32m    623\u001b[0m                     \u001b[0midx\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mset_postfix\u001b[0;34m(self, ordered_dict, refresh, **kwargs)\u001b[0m\n\u001b[1;32m   1273\u001b[0m                                  for key in postfix.keys())\n\u001b[1;32m   1274\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1275\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1276\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1277\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_postfix_str\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrefresh\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mrefresh\u001b[0;34m(self, nolock)\u001b[0m\n\u001b[1;32m   1198\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mnolock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mdisplay\u001b[0;34m(self, msg, pos)\u001b[0m\n\u001b[1;32m   1321\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1323\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmsg\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1324\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mpos\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1325\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmoveto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mpos\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mprint_status\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mprint_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    249\u001b[0m             \u001b[0mlen_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\r'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m             \u001b[0mlast_len\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen_s\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/site-packages/tqdm/_tqdm.py\u001b[0m in \u001b[0;36mfp_write\u001b[0;34m(s)\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mfp_write\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    243\u001b[0m             \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 244\u001b[0;31m             \u001b[0mfp_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    245\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         \u001b[0mlast_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/site-packages/ipykernel/iostream.py\u001b[0m in \u001b[0;36mflush\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    319\u001b[0m             \u001b[0mevt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEvent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    320\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpub_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 321\u001b[0;31m             \u001b[0mevt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    322\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flush\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/vivek/anaconda3/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent('weather_domain.yml', policies = [MemoizationPolicy(), KerasPolicy()])\n",
    "\n",
    "agent.train(\n",
    "training_data_file,\n",
    "augmentation_factor = 50,\n",
    "#max_history = 2,\n",
    "epochs = 500,\n",
    "batch_size = 10,\n",
    "validation_split = 0.2)\n",
    "\n",
    "agent.persist(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train online\n",
    "- all the files save in above step will be required to create a dialogue management model which we will launch in online training session\n",
    "    - online training session will improve our chatbot\n",
    "- ConsoleInputChannel : online trainig session is an actual conversation with a chatbot. we will send messages using console\n",
    "- RasaNLUModel : we will load our model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "from __future__ import unicode_literals\n",
    "\n",
    "import logging\n",
    "\n",
    "from rasa_core.agent import Agent\n",
    "from rasa_core.channels.console import ConsoleInputChannel\n",
    "from rasa_core.interpreter import RegexInterpreter\n",
    "from rasa_core.policies.keras_policy import KerasPolicy\n",
    "from rasa_core.policies.memoization import MemoizationPolicy\n",
    "from rasa_core.interpreter import RasaNLUInterpreter\n",
    "\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def run_weather_online(input_channel, interpreter,\n",
    "                          domain_file=\"weather_domain.yml\",\n",
    "                          training_data_file='data/stories.md'):\n",
    "    agent = Agent(domain_file,\n",
    "                  policies=[MemoizationPolicy(), KerasPolicy()],\n",
    "                  interpreter=interpreter)\n",
    "\n",
    "    agent.train_online(training_data_file,\n",
    "                       input_channel=input_channel,\n",
    "                      # max_history=2,\n",
    "                       batch_size=50,\n",
    "                       epochs=200,\n",
    "                       max_training_samples=300)\n",
    "    return agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logging.basicConfig(level=\"INFO\")\n",
    "    nlu_interpreter = RasaNLUInterpreter('./models/nlu/default/weathernlu')\n",
    "    run_weather_online(ConsoleInputChannel(), nlu_interpreter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from rasa_core.channels import "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
